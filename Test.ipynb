{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/how-to-create-a-custom-gym-environment-with-multiple-agents-f368d13582ee  \n",
    "https://medium.com/analytics-vidhya/custom-gym-environment-with-agents-that-collaborate-4f96ef898a2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv(gym.Env):\n",
    "    def __init__(self, width=10, height=12):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=3,\n",
    "                                            shape=(height, width),\n",
    "                                            dtype=np.int16)\n",
    "        self.reward_range = (-200, 200)\n",
    "        self.current_episode = 0\n",
    "        self.success_episode = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.max_step = 30\n",
    "        \n",
    "        self.state = \"P\"\n",
    "        \n",
    "        self.world = np.zeros((self.height, self.width), dtype=int)\n",
    "        \n",
    "        # Set exit \n",
    "        self.exit_location = [self.height-1, self.width-1]\n",
    "        self.world[self.exit_location[0], self.exit_location[1]] = 2\n",
    "\n",
    "        # Set 5 traps\n",
    "        self.trap_locations = []\n",
    "        for i in range(5):\n",
    "            zero_locations = np.where(self.world == 0)\n",
    "            random_index = np.random.randint(len(indices[0]))\n",
    "            trap_location = [zero_locations[0][random_index], zero_locations[1][random_index]]\n",
    "            self.trap_locations.append(trap_location)\n",
    "            self.world[trap_location[0], trap_location[1]] = 3\n",
    "            \n",
    "\n",
    "        # set player\n",
    "        zero_locations = np.where(self.world == 0)\n",
    "        random_index = np.random.randint(len(indices[0]))\n",
    "        self.player_location = [zero_locations[0][random_index], zero_locations[1][random_index]]\n",
    "        self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "\n",
    "        return self.world\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 3 and self.player_location[1] != 0 : # left\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 2 and self.player_location[0] != (self.height-1) : # down\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 1 and self.player_location[1] != (self.width-1) : # right\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 0 and self.player_location[0] != 0 : # up\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.player_location == self.exit_location:\n",
    "            self.state = \"W\"\n",
    "#             print(f'You won!')\n",
    "            reward = 200\n",
    "            done = True\n",
    "        elif self.player_location in self.trap_locations:\n",
    "            self.state = \"L\"\n",
    "#             print(f'You lost - Stepped on a trap...')\n",
    "            reward = -200\n",
    "            done = True \n",
    "        elif self.current_step == self.max_step:\n",
    "            self.state = \"L\"\n",
    "#             print(\"You lost - Didn't make it in time...\")\n",
    "            reward = -200\n",
    "            done = True    \n",
    "        elif self.state == 'P':\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        obs = self.world\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.world)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| approxkl           | 0.049079027  |\n",
      "| clipfrac           | 0.5234375    |\n",
      "| explained_variance | 0.00144      |\n",
      "| fps                | 369          |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 1.3408334    |\n",
      "| policy_loss        | -0.052190557 |\n",
      "| serial_timesteps   | 128          |\n",
      "| time_elapsed       | 0            |\n",
      "| total_timesteps    | 128          |\n",
      "| value_loss         | 9128.413     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.019682202   |\n",
      "| clipfrac           | 0.21484375    |\n",
      "| explained_variance | -0.00135      |\n",
      "| fps                | 1662          |\n",
      "| n_updates          | 10            |\n",
      "| policy_entropy     | 0.93388456    |\n",
      "| policy_loss        | 0.00067575416 |\n",
      "| serial_timesteps   | 1280          |\n",
      "| time_elapsed       | 1.08          |\n",
      "| total_timesteps    | 1280          |\n",
      "| value_loss         | 6125.9043     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.037868693 |\n",
      "| clipfrac           | 0.34375     |\n",
      "| explained_variance | 0.0246      |\n",
      "| fps                | 1560        |\n",
      "| n_updates          | 20          |\n",
      "| policy_entropy     | 0.74458545  |\n",
      "| policy_loss        | 0.021743868 |\n",
      "| serial_timesteps   | 2560        |\n",
      "| time_elapsed       | 1.98        |\n",
      "| total_timesteps    | 2560        |\n",
      "| value_loss         | 7565.3955   |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| approxkl           | 0.06627618 |\n",
      "| clipfrac           | 0.2109375  |\n",
      "| explained_variance | 0.00745    |\n",
      "| fps                | 1280       |\n",
      "| n_updates          | 30         |\n",
      "| policy_entropy     | 0.2612065  |\n",
      "| policy_loss        | 0.03733661 |\n",
      "| serial_timesteps   | 3840       |\n",
      "| time_elapsed       | 2.78       |\n",
      "| total_timesteps    | 3840       |\n",
      "| value_loss         | 6279.387   |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.093780525 |\n",
      "| clipfrac           | 0.28710938  |\n",
      "| explained_variance | 0.067       |\n",
      "| fps                | 1292        |\n",
      "| n_updates          | 40          |\n",
      "| policy_entropy     | 0.6170712   |\n",
      "| policy_loss        | 0.02610555  |\n",
      "| serial_timesteps   | 5120        |\n",
      "| time_elapsed       | 3.65        |\n",
      "| total_timesteps    | 5120        |\n",
      "| value_loss         | 5112.9224   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.05475338   |\n",
      "| clipfrac           | 0.515625     |\n",
      "| explained_variance | 0.218        |\n",
      "| fps                | 1641         |\n",
      "| n_updates          | 50           |\n",
      "| policy_entropy     | 1.0266242    |\n",
      "| policy_loss        | -0.041855924 |\n",
      "| serial_timesteps   | 6400         |\n",
      "| time_elapsed       | 4.58         |\n",
      "| total_timesteps    | 6400         |\n",
      "| value_loss         | 3123.3992    |\n",
      "-------------------------------------\n",
      "-----------------------------------\n",
      "| approxkl           | 0.0636767  |\n",
      "| clipfrac           | 0.4609375  |\n",
      "| explained_variance | -0.14      |\n",
      "| fps                | 1579       |\n",
      "| n_updates          | 60         |\n",
      "| policy_entropy     | 0.73601353 |\n",
      "| policy_loss        | 0.04902026 |\n",
      "| serial_timesteps   | 7680       |\n",
      "| time_elapsed       | 5.42       |\n",
      "| total_timesteps    | 7680       |\n",
      "| value_loss         | 6525.1123  |\n",
      "-----------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.011029385  |\n",
      "| clipfrac           | 0.15429688   |\n",
      "| explained_variance | -0.0078      |\n",
      "| fps                | 1599         |\n",
      "| n_updates          | 70           |\n",
      "| policy_entropy     | 0.6255038    |\n",
      "| policy_loss        | -0.010545471 |\n",
      "| serial_timesteps   | 8960         |\n",
      "| time_elapsed       | 6.28         |\n",
      "| total_timesteps    | 8960         |\n",
      "| value_loss         | 6863.6943    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x24b60fb4088>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env = DummyVecEnv([lambda: MazeEnv()])\n",
    "model = PPO2(MlpPolicy, env, learning_rate=0.01, verbose=1)\n",
    "model.learn(10_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pygame_exit():\n",
    "    \"\"\" Easy exit from pygame when closing. It will crash otherwise \"\"\"\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 1]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 3 3 0 0 0 0 3 3 0]\n",
      " [0 0 0 0 0 0 1 0 0 3]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    if dones:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros([5, 6], dtype=int); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coord_to_matrix(point, matrix):\n",
    "    return (matrix.shape[0] - point[1] -1, point[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = [3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[(a.shape[0] - point[1] -1, point[0])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeMultiAgentEnv(gym.Env):\n",
    "    def __init__(self, width=10, height=12):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=2,\n",
    "                                            shape=(height, width),\n",
    "                                            dtype=np.int16)\n",
    "        self.reward_range = (-200, 200)\n",
    "        self.current_episode = 0\n",
    "        self.success_episode = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.max_step = 30\n",
    "        \n",
    "        self.state = \"P\"\n",
    "        \n",
    "        self.world = np.zeros((self.height, self.width), dtype=int)\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            self.player_one_location = [np.random.randint(self.height-2), np.random.randint(self.width-2)]\n",
    "            if self.world[self.player_one_location[0], self.player_one_location[1]] == 0:\n",
    "                self.world[self.player_one_location[0], self.player_one_location[1]] == 1\n",
    "                break\n",
    "                \n",
    "        while True:\n",
    "            self.player_two_location = [np.random.randint(self.height-2), np.random.randint(self.width-2)]\n",
    "            if self.world[self.player_two_location[0], self.player_two_location[1]] == 0:\n",
    "                self.world[self.player_two_location[0], self.player_two_location[1]] == 1\n",
    "                break        \n",
    "        \n",
    "        \n",
    "        self.exit_location = [self.height-1, self.width-1]\n",
    "        \n",
    "        self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "        self.world[self.exit_location[0], self.exit_location[1]] = 2\n",
    "        \n",
    "        return self.world\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 3 and self.player_location[1] != 0 : # left\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 2 and self.player_location[0] != (self.height-1) : # down\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 1 and self.player_location[1] != (self.width-1) : # right\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 0 and self.player_location[0] != 0 : # up\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.player_location == self.exit_location:\n",
    "            self.state = \"W\"\n",
    "            print(f'You won!')\n",
    "            reward = 200\n",
    "            done = True\n",
    "        elif self.current_step == self.max_step:\n",
    "            self.state = \"L\"\n",
    "            print(f'You lost')\n",
    "            reward = -200\n",
    "            done = True    \n",
    "        elif self.state == 'P':\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        obs = self.world\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.world)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\common\\policies.py:562: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "[[1 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 0 2]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 2 0 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 2 0 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'render/render.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-ef346983e735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMazeEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMlpPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, seed, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    332\u001b[0m                 \u001b[0mcliprange_vf_now\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcliprange_vf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                 \u001b[1;31m# true_reward is the reward without discount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[0mep_info_buf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_infos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[0mmaybe_ep_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'episode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \"\"\"\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;31m# save final observation where user can get it, then reset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-da812867d629>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-da812867d629>\u001b[0m in \u001b[0;36mrender_episode\u001b[1;34m(self, win_or_lose)\u001b[0m\n\u001b[0;32m    156\u001b[0m             'Success' if win_or_lose == 'W' else 'Failure')\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'render/render.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------------------------------------------\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Episode number {self.current_episode}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'render/render.txt'"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: MazeEnv()])\n",
    "model = PPO2(MlpPolicy, env, learning_rate=0.001)\n",
    "model.learn(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913651379178319"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
