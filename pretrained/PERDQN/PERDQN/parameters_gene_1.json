{
    "__doc__": " Prioritized Experience Replay Deep Q Network\n\n    Parameters:\n    -----------\n    input_dim : int\n        The input dimension\n\n    output_dim : int\n        The output dimension\n\n    explore_step : int, default 5_000\n        The number of epochs until which to decrease epsilon\n\n    train_freq : int, default 20\n        The frequency at which to train the agent\n\n    learning_rate : float, default 0.001\n        Learning rate\n\n    batch_size : int\n        The number of training samples to work through before the model's internal parameters are updated.\n\n    gamma : float, default 0.98\n        Discount factor. How far out should rewards in the future influence the policy?\n\n    capacity : int, default 10_000\n        Capacity of replay buffer\n\n    load_model : str, default False\n        Path to an existing model\n\n    training : bool, default True,\n        Whether to continue training or not\n    ",
    "__module__": "TheGame.Models.PERDQN",
    "_method": "PERDQN",
    "action_size": 8,
    "batch_size": 64,
    "discount_factor": 0.99,
    "epsilon": 0.009999999999947773,
    "epsilon_decay": 0.000198,
    "epsilon_min": 0.01,
    "explore_step": 5000,
    "input_dim": 153,
    "learning_rate": 0.001,
    "memory_size": 20000,
    "method": "PERDQN",
    "one": 1,
    "output_dim": 8,
    "state_size": 153,
    "train_freq": 10,
    "train_start": 1000,
    "training": true
}