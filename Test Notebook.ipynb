{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/how-to-create-a-custom-gym-environment-with-multiple-agents-f368d13582ee  \n",
    "https://medium.com/analytics-vidhya/custom-gym-environment-with-agents-that-collaborate-4f96ef898a2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv(gym.Env):\n",
    "    def __init__(self, width=10, height=12):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=3,\n",
    "                                            shape=(height, width),\n",
    "                                            dtype=np.int16)\n",
    "        self.reward_range = (-200, 200)\n",
    "        self.current_episode = 0\n",
    "        self.success_episode = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.max_step = 30\n",
    "        \n",
    "        self.state = \"P\"\n",
    "        \n",
    "        self.world = np.zeros((self.height, self.width), dtype=int)\n",
    "        \n",
    "        # Set exit \n",
    "        self.exit_location = [self.height-1, self.width-1]\n",
    "        self.world[self.exit_location[0], self.exit_location[1]] = 2\n",
    "\n",
    "        # Set 5 traps\n",
    "        self.trap_locations = []\n",
    "        for i in range(5):\n",
    "            zero_locations = np.where(self.world == 0)\n",
    "            random_index = np.random.randint(len(indices[0]))\n",
    "            trap_location = [zero_locations[0][random_index], zero_locations[1][random_index]]\n",
    "            self.trap_locations.append(trap_location)\n",
    "            self.world[trap_location[0], trap_location[1]] = 3\n",
    "            \n",
    "\n",
    "        # set player\n",
    "        zero_locations = np.where(self.world == 0)\n",
    "        random_index = np.random.randint(len(indices[0]))\n",
    "        self.player_location = [zero_locations[0][random_index], zero_locations[1][random_index]]\n",
    "        self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "\n",
    "        return self.world\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 3 and self.player_location[1] != 0 : # left\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 2 and self.player_location[0] != (self.height-1) : # down\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 1 and self.player_location[1] != (self.width-1) : # right\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 0 and self.player_location[0] != 0 : # up\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.player_location == self.exit_location:\n",
    "            self.state = \"W\"\n",
    "#             print(f'You won!')\n",
    "            reward = 200\n",
    "            done = True\n",
    "        elif self.player_location in self.trap_locations:\n",
    "            self.state = \"L\"\n",
    "#             print(f'You lost - Stepped on a trap...')\n",
    "            reward = -200\n",
    "            done = True \n",
    "        elif self.current_step == self.max_step:\n",
    "            self.state = \"L\"\n",
    "#             print(\"You lost - Didn't make it in time...\")\n",
    "            reward = -200\n",
    "            done = True    \n",
    "        elif self.state == 'P':\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        obs = self.world\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.world)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| approxkl           | 0.049079027  |\n",
      "| clipfrac           | 0.5234375    |\n",
      "| explained_variance | 0.00144      |\n",
      "| fps                | 369          |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 1.3408334    |\n",
      "| policy_loss        | -0.052190557 |\n",
      "| serial_timesteps   | 128          |\n",
      "| time_elapsed       | 0            |\n",
      "| total_timesteps    | 128          |\n",
      "| value_loss         | 9128.413     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.019682202   |\n",
      "| clipfrac           | 0.21484375    |\n",
      "| explained_variance | -0.00135      |\n",
      "| fps                | 1662          |\n",
      "| n_updates          | 10            |\n",
      "| policy_entropy     | 0.93388456    |\n",
      "| policy_loss        | 0.00067575416 |\n",
      "| serial_timesteps   | 1280          |\n",
      "| time_elapsed       | 1.08          |\n",
      "| total_timesteps    | 1280          |\n",
      "| value_loss         | 6125.9043     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.037868693 |\n",
      "| clipfrac           | 0.34375     |\n",
      "| explained_variance | 0.0246      |\n",
      "| fps                | 1560        |\n",
      "| n_updates          | 20          |\n",
      "| policy_entropy     | 0.74458545  |\n",
      "| policy_loss        | 0.021743868 |\n",
      "| serial_timesteps   | 2560        |\n",
      "| time_elapsed       | 1.98        |\n",
      "| total_timesteps    | 2560        |\n",
      "| value_loss         | 7565.3955   |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| approxkl           | 0.06627618 |\n",
      "| clipfrac           | 0.2109375  |\n",
      "| explained_variance | 0.00745    |\n",
      "| fps                | 1280       |\n",
      "| n_updates          | 30         |\n",
      "| policy_entropy     | 0.2612065  |\n",
      "| policy_loss        | 0.03733661 |\n",
      "| serial_timesteps   | 3840       |\n",
      "| time_elapsed       | 2.78       |\n",
      "| total_timesteps    | 3840       |\n",
      "| value_loss         | 6279.387   |\n",
      "-----------------------------------\n",
      "------------------------------------\n",
      "| approxkl           | 0.093780525 |\n",
      "| clipfrac           | 0.28710938  |\n",
      "| explained_variance | 0.067       |\n",
      "| fps                | 1292        |\n",
      "| n_updates          | 40          |\n",
      "| policy_entropy     | 0.6170712   |\n",
      "| policy_loss        | 0.02610555  |\n",
      "| serial_timesteps   | 5120        |\n",
      "| time_elapsed       | 3.65        |\n",
      "| total_timesteps    | 5120        |\n",
      "| value_loss         | 5112.9224   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.05475338   |\n",
      "| clipfrac           | 0.515625     |\n",
      "| explained_variance | 0.218        |\n",
      "| fps                | 1641         |\n",
      "| n_updates          | 50           |\n",
      "| policy_entropy     | 1.0266242    |\n",
      "| policy_loss        | -0.041855924 |\n",
      "| serial_timesteps   | 6400         |\n",
      "| time_elapsed       | 4.58         |\n",
      "| total_timesteps    | 6400         |\n",
      "| value_loss         | 3123.3992    |\n",
      "-------------------------------------\n",
      "-----------------------------------\n",
      "| approxkl           | 0.0636767  |\n",
      "| clipfrac           | 0.4609375  |\n",
      "| explained_variance | -0.14      |\n",
      "| fps                | 1579       |\n",
      "| n_updates          | 60         |\n",
      "| policy_entropy     | 0.73601353 |\n",
      "| policy_loss        | 0.04902026 |\n",
      "| serial_timesteps   | 7680       |\n",
      "| time_elapsed       | 5.42       |\n",
      "| total_timesteps    | 7680       |\n",
      "| value_loss         | 6525.1123  |\n",
      "-----------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.011029385  |\n",
      "| clipfrac           | 0.15429688   |\n",
      "| explained_variance | -0.0078      |\n",
      "| fps                | 1599         |\n",
      "| n_updates          | 70           |\n",
      "| policy_entropy     | 0.6255038    |\n",
      "| policy_loss        | -0.010545471 |\n",
      "| serial_timesteps   | 8960         |\n",
      "| time_elapsed       | 6.28         |\n",
      "| total_timesteps    | 8960         |\n",
      "| value_loss         | 6863.6943    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x24b60fb4088>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env = DummyVecEnv([lambda: MazeEnv()])\n",
    "model = PPO2(MlpPolicy, env, learning_rate=0.01, verbose=1)\n",
    "model.learn(10_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pygame_exit():\n",
    "    \"\"\" Easy exit from pygame when closing. It will crash otherwise \"\"\"\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 1]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 3 3 0]\n",
      " [0 0 0 3 0 0 0 0 3 0]\n",
      " [0 3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n",
      "[[0 3 3 0 0 0 0 3 3 0]\n",
      " [0 0 0 0 0 0 1 0 0 3]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    if dones:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros([5, 6], dtype=int); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coord_to_matrix(point, matrix):\n",
    "    return (matrix.shape[0] - point[1] -1, point[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = [3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[(a.shape[0] - point[1] -1, point[0])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeMultiAgentEnv(gym.Env):\n",
    "    def __init__(self, width=10, height=12):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=2,\n",
    "                                            shape=(height, width),\n",
    "                                            dtype=np.int16)\n",
    "        self.reward_range = (-200, 200)\n",
    "        self.current_episode = 0\n",
    "        self.success_episode = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.max_step = 30\n",
    "        \n",
    "        self.state = \"P\"\n",
    "        \n",
    "        self.world = np.zeros((self.height, self.width), dtype=int)\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            self.player_one_location = [np.random.randint(self.height-2), np.random.randint(self.width-2)]\n",
    "            if self.world[self.player_one_location[0], self.player_one_location[1]] == 0:\n",
    "                self.world[self.player_one_location[0], self.player_one_location[1]] == 1\n",
    "                break\n",
    "                \n",
    "        while True:\n",
    "            self.player_two_location = [np.random.randint(self.height-2), np.random.randint(self.width-2)]\n",
    "            if self.world[self.player_two_location[0], self.player_two_location[1]] == 0:\n",
    "                self.world[self.player_two_location[0], self.player_two_location[1]] == 1\n",
    "                break        \n",
    "        \n",
    "        \n",
    "        self.exit_location = [self.height-1, self.width-1]\n",
    "        \n",
    "        self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "        self.world[self.exit_location[0], self.exit_location[1]] = 2\n",
    "        \n",
    "        return self.world\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 3 and self.player_location[1] != 0 : # left\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 2 and self.player_location[0] != (self.height-1) : # down\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 1 and self.player_location[1] != (self.width-1) : # right\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[1] += 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        elif action == 0 and self.player_location[0] != 0 : # up\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 0\n",
    "            self.player_location[0] -= 1\n",
    "            self.world[self.player_location[0], self.player_location[1]] = 1\n",
    "            \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.player_location == self.exit_location:\n",
    "            self.state = \"W\"\n",
    "            print(f'You won!')\n",
    "            reward = 200\n",
    "            done = True\n",
    "        elif self.current_step == self.max_step:\n",
    "            self.state = \"L\"\n",
    "            print(f'You lost')\n",
    "            reward = -200\n",
    "            done = True    \n",
    "        elif self.state == 'P':\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        obs = self.world\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.world)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\common\\policies.py:562: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "[[1 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 0 2]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 2 0 0]\n",
      " [0 0 1 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 2 0 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 1 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[1 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [0 1 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n",
      "[[0 0 2 0]\n",
      " [1 0 0 0]\n",
      " [0 3 4 3]\n",
      " [0 4 0 0]]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'render/render.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-ef346983e735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMazeEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMlpPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, seed, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    332\u001b[0m                 \u001b[0mcliprange_vf_now\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcliprange_vf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                 \u001b[1;31m# true_reward is the reward without discount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[0mep_info_buf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_infos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[0mmaybe_ep_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'episode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \"\"\"\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;31m# save final observation where user can get it, then reset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-da812867d629>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-da812867d629>\u001b[0m in \u001b[0;36mrender_episode\u001b[1;34m(self, win_or_lose)\u001b[0m\n\u001b[0;32m    156\u001b[0m             'Success' if win_or_lose == 'W' else 'Failure')\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'render/render.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------------------------------------------\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Episode number {self.current_episode}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'render/render.txt'"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: MazeEnv()])\n",
    "model = PPO2(MlpPolicy, env, learning_rate=0.001)\n",
    "model.learn(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913651379178319"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fov(some_grid, point):\n",
    "    player = (2, 2)\n",
    "    x = point[0] - player[0]\n",
    "    y = player[1] - point[1]\n",
    "    some_grid[2+y, 2+x] = 1\n",
    "    return some_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 9., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = np.zeros([5, 5])\n",
    "grid[2, 2] = 9; grid\n",
    "grid = update_fov(grid, (1, 2)); grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = np.zeros([2, 5, 5]); grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid[0][-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    grid[0][i, 0] = 1\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 9., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = list(itertools.product(range(grid.shape[0]), range(grid.shape[0]))); c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "player = (2, 2)\n",
    "list1=[0, 1, 2, 3, 4]\n",
    "c = list(itertools.product(list1, list1))\n",
    "d = [a for a in c if abs(a[0] - player[0]) + abs(a[1] - player[1]) <= 2]\n",
    "d.remove((2, 2))\n",
    "\n",
    "# fov = [grid[i] for i in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 9., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_coordinates = [(i, j) for i in range(5) for j in range(5) if abs(i - player[0]) + abs(j - player[1]) <= 2]\n",
    "selected_coordinates.remove((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (2, 2)\n",
    "t1 = (2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t1[0] - c[0], c[1] - t1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = np.zeros([5,5]); grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = (2, 2)\n",
    "point = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 9., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid[2+-2, 2+0] = 1; grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, x, y, value):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.value = value\n",
    "        self.coordinates = np.array([x, y], dtype=int)\n",
    "\n",
    "    def update(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.coordinates = np.array([x, y], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = Point(1, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = food.value / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spaces.Box(low=-1, high=1, shape=(4, ), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common import make_vec_env\n",
    "from Field.EnvironmentTest import Environment as MazeEnv\n",
    "\n",
    "env = make_vec_env(MazeEnv, n_envs=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\stable_baselines\\common\\policies.py:560: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\maarten.grootendorst\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00027311133 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 21            |\n",
      "| ep_reward_mean     | -400          |\n",
      "| explained_variance | -7.8e-05      |\n",
      "| fps                | 1279          |\n",
      "| n_updates          | 1             |\n",
      "| policy_entropy     | 1.385964      |\n",
      "| policy_loss        | -0.0016631782 |\n",
      "| serial_timesteps   | 128           |\n",
      "| time_elapsed       | 0             |\n",
      "| total_timesteps    | 512           |\n",
      "| value_loss         | 33101.582     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x20814c41c88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.common import make_vec_env\n",
    "\n",
    "from stable_baselines import PPO2, DQN, SAC\n",
    "from Field.EnvironmentTest import Environment as MazeEnv\n",
    "\n",
    "\n",
    "env = make_vec_env(MazeEnv, n_envs=4, env_kwargs={'width':10, 'height':12})\n",
    "\n",
    "# model = DQN(MlpPolicyDQN, env, learning_rate=0.0001, verbose=1, tensorboard_log=\"logging\")\n",
    "model = PPO2(MlpPolicy, env, learning_rate=0.001, verbose=1, tensorboard_log=\"logging\")\n",
    "\n",
    "model.learn(50_000, log_interval=100)\n",
    "\n",
    "# obs = env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = env.step(action)\n",
    "#     if not env.render():\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| approxkl           | 0.00023086624 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 22            |\n",
      "| ep_reward_mean     | -329          |\n",
      "| explained_variance | -8.07e-05     |\n",
      "| fps                | 1422          |\n",
      "| n_updates          | 1             |\n",
      "| policy_entropy     | 1.386037      |\n",
      "| policy_loss        | -0.0015233436 |\n",
      "| serial_timesteps   | 128           |\n",
      "| time_elapsed       | 0             |\n",
      "| total_timesteps    | 512           |\n",
      "| value_loss         | 32989.25      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00020940232 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 21.7          |\n",
      "| ep_reward_mean     | -347          |\n",
      "| explained_variance | -1.9e-05      |\n",
      "| fps                | 4063          |\n",
      "| n_updates          | 2             |\n",
      "| policy_entropy     | 1.3840895     |\n",
      "| policy_loss        | -2.491707e-05 |\n",
      "| serial_timesteps   | 256           |\n",
      "| time_elapsed       | 0.36          |\n",
      "| total_timesteps    | 1024          |\n",
      "| value_loss         | 38621.246     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00014920958 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 21.6          |\n",
      "| ep_reward_mean     | -361          |\n",
      "| explained_variance | 0.000207      |\n",
      "| fps                | 4266          |\n",
      "| n_updates          | 3             |\n",
      "| policy_entropy     | 1.3837116     |\n",
      "| policy_loss        | 0.00022845523 |\n",
      "| serial_timesteps   | 384           |\n",
      "| time_elapsed       | 0.487         |\n",
      "| total_timesteps    | 1536          |\n",
      "| value_loss         | 35619.14      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| approxkl           | 0.00022355541  |\n",
      "| clipfrac           | 0.0            |\n",
      "| ep_len_mean        | 21.7           |\n",
      "| ep_reward_mean     | -352           |\n",
      "| explained_variance | 0.00045        |\n",
      "| fps                | 4061           |\n",
      "| n_updates          | 4              |\n",
      "| policy_entropy     | 1.3845389      |\n",
      "| policy_loss        | -0.00025729887 |\n",
      "| serial_timesteps   | 512            |\n",
      "| time_elapsed       | 0.608          |\n",
      "| total_timesteps    | 2048           |\n",
      "| value_loss         | 36206.92       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| approxkl           | 0.00019620746  |\n",
      "| clipfrac           | 0.0            |\n",
      "| ep_len_mean        | 21.5           |\n",
      "| ep_reward_mean     | -358           |\n",
      "| explained_variance | 0.000494       |\n",
      "| fps                | 4197           |\n",
      "| n_updates          | 5              |\n",
      "| policy_entropy     | 1.3843613      |\n",
      "| policy_loss        | -7.8282435e-05 |\n",
      "| serial_timesteps   | 640            |\n",
      "| time_elapsed       | 0.735          |\n",
      "| total_timesteps    | 2560           |\n",
      "| value_loss         | 37568.46       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00010661101 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 21.3          |\n",
      "| ep_reward_mean     | -373          |\n",
      "| explained_variance | 5.81e-05      |\n",
      "| fps                | 3820          |\n",
      "| n_updates          | 6             |\n",
      "| policy_entropy     | 1.3837028     |\n",
      "| policy_loss        | -0.0009349809 |\n",
      "| serial_timesteps   | 768           |\n",
      "| time_elapsed       | 0.858         |\n",
      "| total_timesteps    | 3072          |\n",
      "| value_loss         | 32534.936     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00043351104 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 21.2          |\n",
      "| ep_reward_mean     | -382          |\n",
      "| explained_variance | 0.000667      |\n",
      "| fps                | 2828          |\n",
      "| n_updates          | 7             |\n",
      "| policy_entropy     | 1.3792893     |\n",
      "| policy_loss        | 0.0012789095  |\n",
      "| serial_timesteps   | 896           |\n",
      "| time_elapsed       | 0.993         |\n",
      "| total_timesteps    | 3584          |\n",
      "| value_loss         | 44717.402     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00014761613 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 21.3          |\n",
      "| ep_reward_mean     | -373          |\n",
      "| explained_variance | 0.00123       |\n",
      "| fps                | 3626          |\n",
      "| n_updates          | 8             |\n",
      "| policy_entropy     | 1.3788825     |\n",
      "| policy_loss        | -0.0001482906 |\n",
      "| serial_timesteps   | 1024          |\n",
      "| time_elapsed       | 1.18          |\n",
      "| total_timesteps    | 4096          |\n",
      "| value_loss         | 34008.242     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| approxkl           | 1.9530427e-05  |\n",
      "| clipfrac           | 0.0            |\n",
      "| ep_len_mean        | 21.9           |\n",
      "| ep_reward_mean     | -333           |\n",
      "| explained_variance | 0.00132        |\n",
      "| fps                | 3970           |\n",
      "| n_updates          | 9              |\n",
      "| policy_entropy     | 1.3784281      |\n",
      "| policy_loss        | -0.00018563253 |\n",
      "| serial_timesteps   | 1152           |\n",
      "| time_elapsed       | 1.32           |\n",
      "| total_timesteps    | 4608           |\n",
      "| value_loss         | 31180.49       |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.deepq.policies import MlpPolicy as MlpPolicyDQN\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.common import make_vec_env\n",
    "\n",
    "from stable_baselines import PPO2, DQN\n",
    "from Field.EnvironmentTest import Environment as MazeEnv\n",
    "\n",
    "# env = DummyVecEnv([lambda: MazeEnv(width=10, height=12)])\n",
    "env = make_vec_env(MazeEnv, n_envs=4, env_kwargs=dict(width=10, height=12))\n",
    "\n",
    "# model = DQN(MlpPolicyDQN, env, learning_rate=0.0001, verbose=1, tensorboard_log=\"logging\")\n",
    "model = PPO2(MlpPolicy, env, learning_rate=0.001, verbose=1)\n",
    "\n",
    "model.learn(5_000, log_interval=1)\n",
    "\n",
    "\n",
    "env = MazeEnv(width=10, height=12)\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    if not env.render():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Render not defined for <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000208186DD308>\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 0, 1, 1, 1, 1, 1, 2, 3],\n",
       "       [0, 0, 2, 1, 3, 2, 1, 1, 1, 3],\n",
       "       [1, 3, 1, 1, 3, 1, 0, 0, 3, 0],\n",
       "       [3, 3, 0, 1, 1, 2, 1, 1, 1, 3],\n",
       "       [2, 1, 0, 2, 0, 2, 2, 2, 3, 0],\n",
       "       [0, 0, 3, 1, 3, 3, 3, 0, 2, 1],\n",
       "       [0, 3, 0, 2, 1, 2, 3, 0, 1, 2],\n",
       "       [0, 2, 2, 0, 0, 2, 3, 0, 0, 2],\n",
       "       [3, 1, 1, 2, 2, 2, 2, 0, 0, 2],\n",
       "       [0, 0, 0, 1, 0, 0, 3, 1, 2, 2],\n",
       "       [3, 3, 3, 3, 3, 3, 0, 1, 3, 1],\n",
       "       [3, 2, 1, 3, 2, 2, 0, 1, 3, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = np.random.randint(0, 4, (12, 10)); thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
