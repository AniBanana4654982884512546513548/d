{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TheGame.Models.ppo2c import PPO\n",
    "from TheGame import Environment\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = 102\n",
    "action_dim = 8\n",
    "render = False\n",
    "solved_reward = 230  # stop training if avg_reward > solved_reward\n",
    "log_interval = 20  # print avg reward in the interval\n",
    "max_episodes = 50000  # max training episodes\n",
    "n_latent_var = 64  # number of variables in hidden layer\n",
    "update_timestep = 100  # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99  # discount factor\n",
    "K_epochs = 4  # update policy for K epochs\n",
    "eps_clip = 0.2  # clip parameter for PPO\n",
    "random_seed = None\n",
    "nr_agents = 2\n",
    "best_agent = 0\n",
    "best_score = -10_000\n",
    "print_interval = 100\n",
    "scores = [0.0 for _ in range(nr_agents)]\n",
    "\n",
    "# Init env\n",
    "brains = [PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip) for _ in range(nr_agents)]\n",
    "env = Environment(width=20, height=20, nr_agents=nr_agents, evolution=True, fps=20, brains=brains, grid_size=24)\n",
    "env.max_step = 30_000\n",
    "s = env.reset()\n",
    "\n",
    "good = 0\n",
    "error = 0\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for i, agent in enumerate(env.agents):\n",
    "    (action, agent.brain.states,\n",
    "     agent.brain.actions, agent.brain.logprobs) = agent.brain.policy_old.act(s[i],\n",
    "                                                                             agent.brain.states,\n",
    "                                                                             agent.brain.actions,\n",
    "                                                                             agent.brain.logprobs)\n",
    "    actions.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s_prime, r, dones, infos = env.step(actions)\n",
    "\n",
    "# Learn only if still alive (not done)\n",
    "for i, agent in enumerate(env.agents):\n",
    "    if agent.fitness > best_score:\n",
    "        best_score = agent.fitness\n",
    "\n",
    "    agent.brain.rewards.append(r[i])\n",
    "    agent.brain.is_terminals.append(dones[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0000,  0.0000, -1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  1.0000,  0.9500,  0.9500,  0.0000])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents[0].brain.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -1.0000,  1.0000, -1.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  1.0000,  0.3500,  0.4500,  0.0000])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents[1].brain.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################\n",
    "#  ONLY UPDATES ONE AGENT....  #\n",
    "################################\n",
    "for i, agent in enumerate(env.agents):\n",
    "    if t % update_timestep == 0 and t != 0:\n",
    "        # print(i, len(agent.brain.memory.states))\n",
    "        agent.brain.update()\n",
    "        agent.brain.clear_memory()\n",
    "        good += 1\n",
    "\n",
    "if t % update_timestep == 0 and t != 0:\n",
    "    t = 0\n",
    "\n",
    "s = env.update_env()\n",
    "\n",
    "if n_epi % 1000 == 0:\n",
    "    print(f\"Best score: {best_score}. Nr episodes: {n_epi}. Nr_agents: {len(env.agents)}. Good: {good}. Errors\"\n",
    "          f\": {error}\" )\n",
    "t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TheGame.Models.ppo2c import PPO\n",
    "from TheGame import Environment\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = 102\n",
    "action_dim = 8\n",
    "render = False\n",
    "solved_reward = 230  # stop training if avg_reward > solved_reward\n",
    "log_interval = 20  # print avg reward in the interval\n",
    "max_episodes = 50000  # max training episodes\n",
    "n_latent_var = 64  # number of variables in hidden layer\n",
    "update_timestep = 100  # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99  # discount factor\n",
    "K_epochs = 4  # update policy for K epochs\n",
    "eps_clip = 0.2  # clip parameter for PPO\n",
    "random_seed = None\n",
    "nr_agents = 1\n",
    "best_agent = 0\n",
    "best_score = -10_000\n",
    "print_interval = 100\n",
    "scores = [0.0 for _ in range(nr_agents)]\n",
    "\n",
    "# Init env\n",
    "brains = [PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip) for _ in range(nr_agents)]\n",
    "env = Environment(width=20, height=20, nr_agents=nr_agents, evolution=True, fps=20, brains=brains, grid_size=24)\n",
    "env.max_step = 30_000\n",
    "s = env.reset()\n",
    "\n",
    "good = 0\n",
    "error = 0\n",
    "t = 0\n",
    "for n_epi in range(30_000):\n",
    "    # actions = [agent.brain.policy_old.act(s[i], agent.brain.memory) for i, agent in enumerate(env.agents)]\n",
    "\n",
    "    actions = []\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        (action, agent.brain.states,\n",
    "         agent.brain.actions, agent.brain.logprobs) = agent.brain.policy_old.act(s[i],\n",
    "                                                                                 agent.brain.states,\n",
    "                                                                                 agent.brain.actions,\n",
    "                                                                                 agent.brain.logprobs)\n",
    "        actions.append(action)\n",
    "\n",
    "    s_prime, r, dones, infos = env.step(actions)\n",
    "\n",
    "    # Learn only if still alive (not done)\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        if agent.fitness > best_score:\n",
    "            best_score = agent.fitness\n",
    "\n",
    "        agent.brain.rewards.append(r[i])\n",
    "        agent.brain.is_terminals.append(dones[i])\n",
    "\n",
    "    ################################\n",
    "    #  ONLY UPDATES ONE AGENT....  #\n",
    "    ################################\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        if t % update_timestep == 0 and t != 0:\n",
    "            # print(i, len(agent.brain.memory.states))\n",
    "            agent.brain.update()\n",
    "            agent.brain.clear_memory()\n",
    "            good += 1\n",
    "\n",
    "    if t % update_timestep == 0 and t != 0:\n",
    "        t = 0\n",
    "\n",
    "    s = env.update_env()\n",
    "\n",
    "    if n_epi % 1000 == 0:\n",
    "        print(f\"Best score: {best_score}. Nr episodes: {n_epi}. Nr_agents: {len(env.agents)}. Good: {good}. Errors\"\n",
    "              f\": {error}\" )\n",
    "    t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
