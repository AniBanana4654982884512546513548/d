{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from TheGame import Environment\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = Environment(width=7, height=7, nr_agents=3, grid_size=24)\n",
    "env.reset()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 2., 0., 3., 0., 0.],\n",
       "       [0., 0., 0., 0., 2., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 3., 0., 0., 0.],\n",
       "       [2., 0., 2., 1., 0., 0., 2.],\n",
       "       [1., 0., 0., 0., 2., 0., 0.],\n",
       "       [0., 3., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.grid.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 1., 0., 0., 2., 2.],\n",
       "       [0., 0., 0., 2., 0., 0., 1.],\n",
       "       [3., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 2., 0., 3., 0., 0., 0.],\n",
       "       [0., 0., 0., 2., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 0., 3., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = env.grid.fov(*env.agents[0].coordinates, 3); test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., -1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._get_obs_entities(env.agents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    env = Environment(width=7, height=7, nr_agents=3, grid_size=24)\n",
    "    env.reset()\n",
    "    test = env.grid.fov(*env.agents[1].coordinates, 3); test\n",
    "\n",
    "    for i in range(3):\n",
    "        diff_x = np.where(test == 3)[0][2] - 3\n",
    "        diff_y = np.where(test == 3)[1][2] - 3\n",
    "\n",
    "        # Get coordinates if within normal range\n",
    "        global_x = env.agents[1].x + diff_x\n",
    "        global_y = env.agents[1].y + diff_y\n",
    "\n",
    "        # Get coordinates if through wall (left vs right)\n",
    "        if global_y < 0:\n",
    "            global_y = env.width + global_y\n",
    "        elif global_y >= env.width:\n",
    "            global_y = global_y - env.width\n",
    "\n",
    "        # # Get coordinates if through wall (up vs down)\n",
    "        if global_x < 0:\n",
    "            global_x = env.height + global_x\n",
    "        elif global_x >= env.height:\n",
    "            global_x = global_x - env.height\n",
    "\n",
    "    #     print(global_x, global_y)\n",
    "    #     print(env.grid.grid[global_x, global_y])  \n",
    "        try:\n",
    "            if not env.grid.grid[global_x, global_y] == 3:\n",
    "                print('Aaaw....')\n",
    "        except:\n",
    "            print(global_x, global_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, -2)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_x, global_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_x = np.where(test == 3)[0][0] - 3\n",
    "diff_y = np.where(test == 3)[1][0] - 3\n",
    "\n",
    "# Get coordinates if within normal range\n",
    "if 0 <= env.agents[0].x + diff_x < env.width:\n",
    "    global_x = env.agents[0].x + diff_x\n",
    "if 0 <= env.agents[0].y + diff_y < env.height:\n",
    "    global_y = env.agents[0].y + diff_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents[0].coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = env.agents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.grid.fov(agent.x, agent.y, 3)\n",
    "loc_agents = np.where(observation == env.entities.agent)\n",
    "family = np.zeros([7, 7])\n",
    "for i_local, j_local in zip(loc_agents[0], loc_agents[1]):\n",
    "    i_global, j_global = agent.x + i_local - 3, agent.y + j_local - 3\n",
    "    for other_agent in env.agents:\n",
    "        if other_agent.coordinates == [i_global, j_global]:\n",
    "            if other_agent.gen == agent.gen:\n",
    "                family[i_local, j_local] = 1\n",
    "            else:\n",
    "                family[i_local, j_local] = -1\n",
    "\n",
    "family_flat = list(family.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TheGame.Models.ppo2c import PPO\n",
    "from TheGame import Environment\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = 102\n",
    "action_dim = 8\n",
    "render = False\n",
    "solved_reward = 230  # stop training if avg_reward > solved_reward\n",
    "log_interval = 20  # print avg reward in the interval\n",
    "max_episodes = 50000  # max training episodes\n",
    "n_latent_var = 64  # number of variables in hidden layer\n",
    "update_timestep = 100  # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99  # discount factor\n",
    "K_epochs = 4  # update policy for K epochs\n",
    "eps_clip = 0.2  # clip parameter for PPO\n",
    "random_seed = None\n",
    "nr_agents = 2\n",
    "best_agent = 0\n",
    "best_score = -10_000\n",
    "print_interval = 100\n",
    "scores = [0.0 for _ in range(nr_agents)]\n",
    "\n",
    "# Init env\n",
    "brains = [PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip) for _ in range(nr_agents)]\n",
    "env = Environment(width=20, height=20, nr_agents=nr_agents, evolution=True, fps=20, brains=brains, grid_size=24)\n",
    "env.max_step = 30_000\n",
    "s = env.reset()\n",
    "\n",
    "good = 0\n",
    "error = 0\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for i, agent in enumerate(env.agents):\n",
    "    (action, agent.brain.states,\n",
    "     agent.brain.actions, agent.brain.logprobs) = agent.brain.policy_old.act(s[i],\n",
    "                                                                             agent.brain.states,\n",
    "                                                                             agent.brain.actions,\n",
    "                                                                             agent.brain.logprobs)\n",
    "    actions.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s_prime, r, dones, infos = env.step(actions)\n",
    "\n",
    "# Learn only if still alive (not done)\n",
    "for i, agent in enumerate(env.agents):\n",
    "    if agent.fitness > best_score:\n",
    "        best_score = agent.fitness\n",
    "\n",
    "    agent.brain.rewards.append(r[i])\n",
    "    agent.brain.is_terminals.append(dones[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0000,  0.0000, -1.0000, -1.0000,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  1.0000,  0.9500,  0.9500,  0.0000])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents[0].brain.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -1.0000,  1.0000, -1.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  1.0000,  0.3500,  0.4500,  0.0000])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agents[1].brain.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################\n",
    "#  ONLY UPDATES ONE AGENT....  #\n",
    "################################\n",
    "for i, agent in enumerate(env.agents):\n",
    "    if t % update_timestep == 0 and t != 0:\n",
    "        # print(i, len(agent.brain.memory.states))\n",
    "        agent.brain.update()\n",
    "        agent.brain.clear_memory()\n",
    "        good += 1\n",
    "\n",
    "if t % update_timestep == 0 and t != 0:\n",
    "    t = 0\n",
    "\n",
    "s = env.update_env()\n",
    "\n",
    "if n_epi % 1000 == 0:\n",
    "    print(f\"Best score: {best_score}. Nr episodes: {n_epi}. Nr_agents: {len(env.agents)}. Good: {good}. Errors\"\n",
    "          f\": {error}\" )\n",
    "t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TheGame.Models.ppo2c import PPO\n",
    "from TheGame import Environment\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = 102\n",
    "action_dim = 8\n",
    "render = False\n",
    "solved_reward = 230  # stop training if avg_reward > solved_reward\n",
    "log_interval = 20  # print avg reward in the interval\n",
    "max_episodes = 50000  # max training episodes\n",
    "n_latent_var = 64  # number of variables in hidden layer\n",
    "update_timestep = 100  # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99  # discount factor\n",
    "K_epochs = 4  # update policy for K epochs\n",
    "eps_clip = 0.2  # clip parameter for PPO\n",
    "random_seed = None\n",
    "nr_agents = 1\n",
    "best_agent = 0\n",
    "best_score = -10_000\n",
    "print_interval = 100\n",
    "scores = [0.0 for _ in range(nr_agents)]\n",
    "\n",
    "# Init env\n",
    "brains = [PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip) for _ in range(nr_agents)]\n",
    "env = Environment(width=20, height=20, nr_agents=nr_agents, evolution=True, fps=20, brains=brains, grid_size=24)\n",
    "env.max_step = 30_000\n",
    "s = env.reset()\n",
    "\n",
    "good = 0\n",
    "error = 0\n",
    "t = 0\n",
    "for n_epi in range(30_000):\n",
    "    # actions = [agent.brain.policy_old.act(s[i], agent.brain.memory) for i, agent in enumerate(env.agents)]\n",
    "\n",
    "    actions = []\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        (action, agent.brain.states,\n",
    "         agent.brain.actions, agent.brain.logprobs) = agent.brain.policy_old.act(s[i],\n",
    "                                                                                 agent.brain.states,\n",
    "                                                                                 agent.brain.actions,\n",
    "                                                                                 agent.brain.logprobs)\n",
    "        actions.append(action)\n",
    "\n",
    "    s_prime, r, dones, infos = env.step(actions)\n",
    "\n",
    "    # Learn only if still alive (not done)\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        if agent.fitness > best_score:\n",
    "            best_score = agent.fitness\n",
    "\n",
    "        agent.brain.rewards.append(r[i])\n",
    "        agent.brain.is_terminals.append(dones[i])\n",
    "\n",
    "    ################################\n",
    "    #  ONLY UPDATES ONE AGENT....  #\n",
    "    ################################\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        if t % update_timestep == 0 and t != 0:\n",
    "            # print(i, len(agent.brain.memory.states))\n",
    "            agent.brain.update()\n",
    "            agent.brain.clear_memory()\n",
    "            good += 1\n",
    "\n",
    "    if t % update_timestep == 0 and t != 0:\n",
    "        t = 0\n",
    "\n",
    "    s = env.update_env()\n",
    "\n",
    "    if n_epi % 1000 == 0:\n",
    "        print(f\"Best score: {best_score}. Nr episodes: {n_epi}. Nr_agents: {len(env.agents)}. Good: {good}. Errors\"\n",
    "              f\": {error}\" )\n",
    "    t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
